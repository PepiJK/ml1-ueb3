{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uebung 03\n",
    "\n",
    "Gruppe: Josef Koch, Thomas Wally\n",
    "\n",
    "## Anmerkungen\n",
    "\n",
    "Für diese Übung ist es zugelassen, dass Sie in __Gruppen von zwei Personen__ arbeiten.\n",
    "Sie dürfen auch alleine arbeiten wenn Sie das bevorzugen.\n",
    "Beide Gruppenmitglieder müssen die Kreuzerlliste ausfüllen und die Ausarbeitung hochladen.\n",
    "Auch müssen die Namen beider Gruppenmitglieder in der Ausarbeitung vermerkt werden.\n",
    "\n",
    "Geben Sie bitte Quellen an wenn Sie Code aus dem Internet Kopieren.\n",
    "Laden Sie Ihre Lösung nur in privaten Repos auf Platformen wie GitHub, GitLab, usw. hoch.\n",
    "\n",
    "## Decision Tree und Random Forest\n",
    "\n",
    "In dieser Uebung sollen Sie einen Decision Tree mit Hilfe des ID3-Algorithmus selbst implementieren.\n",
    "Die Performance ihrer Implementierung sollen Sie mit der Implementierung einer Bibliothek vergleichen.\n",
    "Im Anschluss sollen Sie die Performance des Decision Trees mit der des Random Forest vergleichen.\n",
    "Zur Klassifizierung verwenden wir dieses Mal den [Breast Cancer Datensatz](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer).\n",
    "Die unten angegebenen Files `breast-cancer_train.data` und `breast-cancer_test.data` finden Sie im Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 03.01 (15 Punkte)\n",
    "\n",
    "Implementieren Sie selbststaendig einen Decision Tree.\n",
    "Verwenden Sie dazu den ID3-Algorithmus.\n",
    "\n",
    "- [ ] Daten einlesen\n",
    "- [ ] Decision Tree implementieren\n",
    "- [ ] Decision Tree erstellen (trainieren mit `breast-cancer_train.data`)\n",
    "- [ ] Accuracy mit `breast-cancer_test.data` berechnen\n",
    "- [ ] Laufzeit des Training des Decision Tree angeben\n",
    "\n",
    "Sie muessen kein Pruning fuer den Baum implementieren.\n",
    "Sie muessen fuer diese Aufgabe keine _k_-Fold-Cross-Validation machen.\n",
    "\n",
    "Implementierungshinweise:\n",
    "* Fuer die Berechnung des Information Gain duerfen Sie Funktionen aus der `math` oder `numpy` Bibliotek verwenden.\n",
    "* Sie duerfen __pandas__(`pandas`) verwenden um die Daten einzulesen und zu speichern.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer die Berechnung der Accuracy verwenden.\n",
    "* Sie duerfen fuer den Decision Tree __keine__ Machine Learning Bibliothek verwenden (ausgenommen der oben angefuehrten)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 03.02 (5 Punkte)\n",
    "\n",
    "Verwenden Sie eine fertige Implementierung des Decision Trees von __scikit-learn__(`sklearn`).\n",
    "\n",
    "- [ ] Daten einlesen\n",
    "- [ ] Decision Tree mit __scikit-learn__(`sklearn`) erstellen (trainieren mit `breast-cancer_train.data`)\n",
    "- [ ] Accuracy mit `breast-cancer_test.data` berechnen\n",
    "- [ ] Verlgeichen Sie die Accuracy Ihrer Implementierung der des Decision Tree den Sie mit __scikit-learn__ erstellt haben. Versuchen Sie den Unterschied in der Accuracy zu erklaeren.\n",
    "\n",
    "Implementierungshinweise:\n",
    "* Sie duerfen __pandas__(`pandas`) verwenden um die Daten einzulesen und zu speichern.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer die Berechnung der Accuracy verwenden.\n",
    "* Sie sollen __scikit-learn__(`sklearn`) fuer den Decision Tree verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aufgabe 03.03 (5 Bonuspunkte)\n",
    "\n",
    "Verwenden Sie eine fertige Implementierung des Random Forest von __scikit-learn__(`sklearn`).\n",
    "\n",
    "- [ ] Daten einlesen\n",
    "- [ ] Random Forest mit __scikit-learn__(`sklearn`) erstellen. Tunen Sie die Hyperparameter mit _k_-Fold-Cross-Validation mit `breast-cancer_train.data`.\n",
    "- [ ] Accuracy mit `breast-cancer_test.data` berechnen\n",
    "- [ ] Verlgeichen Sie die Accuracy des Decision Tree den Sie mit erstellt haben __scikit-learn__ mit der des Random Forest den Sie mit __scikit-learn__ erstellt haben. Versuchen Sie den Unterschied in der Accuracy zu erklaeren.\n",
    "\n",
    "Implementierungshinweise:\n",
    "* Sie duerfen __pandas__(`pandas`) verwenden um die Daten einzulesen und zu speichern.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer die Berechnung der Accuracy verwenden.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer _k_-Fold-Cross-Validation verwenden.\n",
    "* Sie sollen __scikit-learn__(`sklearn`) fuer den Random Forest verwenden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anmerkungen\n",
    "\n",
    "Vergessen Sie nicht ihre Abgabe entsprechend zu dokumentieren.\n",
    "\n",
    "Laden Sie die von Ihnen bearbeitete `.ipynb` Datei sowie ein dazugehoeriges `requirements.txt` File in einer `.zip` Datei im Moodle Kurs hoch.\n",
    "Achten Sie darauf, dass keine Umlaute im Namen ihrer abgegebenen Files enthalten sind."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "#### Data Features\n",
    "1. Class: no-recurrence-events, recurrence-events\n",
    "2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "3. menopause: lt40, ge40, premeno.\n",
    "4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "6. node-caps: yes, no.\n",
    "7. deg-malig: 1, 2, 3.\n",
    "8. breast: left, right.\n",
    "9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
    "10. irradiat: yes, no.\n",
    "\n",
    "### Aufgabe 03.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy = 66.23%\n",
      "Time = 1.89s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "import timeit\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the entropy of a given data frame column\n",
    "# Lecture03.pdf, page 34\n",
    "def calc_entropy(col):\n",
    "    values = col.unique()\n",
    "    ent = 0\n",
    "    for value in values:\n",
    "        # Amount of a specific value / total amount of values\n",
    "        pi = col.value_counts()[value] / len(col)\n",
    "        ent += - pi * math.log2(pi)\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Calculate the information gain of a given data frame and attribute\n",
    "# Lecture03.pdf, page 36\n",
    "def calc_gain(df, attribute, target_attribute = 'class'):\n",
    "    total_ent = calc_entropy(df[target_attribute])\n",
    "    attribute_values = df[attribute].unique()\n",
    "    attribute_ent = 0\n",
    "    for value in attribute_values:\n",
    "        value_df = df.loc[df[attribute] == value]\n",
    "        entropy_df = calc_entropy(value_df[target_attribute])\n",
    "        value_ratio = len(value_df) / len(df)\n",
    "        attribute_ent += value_ratio * entropy_df\n",
    "    return total_ent - attribute_ent\n",
    "\n",
    "\n",
    "# Build the tree using the ID3 algorithm\n",
    "# https://en.wikipedia.org/wiki/ID3_algorithm\n",
    "def id3(df, attributes, target_attribute = 'class'):\n",
    "    \n",
    "    # If there is only one value of the target attribute in the data frame, then return this value\n",
    "    if len(df[target_attribute].unique()) == 1:\n",
    "        return df[target_attribute].unique()[0]\n",
    "    \n",
    "    # If the attributes are empty, then return most common value of the target attribute in the data frame\n",
    "    if len(attributes) == 0:\n",
    "        return df[target_attribute].mode()[0]\n",
    "    \n",
    "    # Otherwise start the ID3 algorithm\n",
    "    # Get the attribute with the highest information gain\n",
    "    gains = {}\n",
    "    for attribute in attributes:\n",
    "        gains[attribute] = calc_gain(df, attribute)\n",
    "    best_attribute = max(gains, key=gains.get)\n",
    "    \n",
    "    # Build the tree\n",
    "    tree = { best_attribute: {} }\n",
    "    \n",
    "    # Remove the best attribute from the attributes array\n",
    "    attributes = [i for i in attributes if i != best_attribute]  \n",
    "    # TODO: Der befehl drunter sollt des gleiche machen wie der oben\n",
    "    # aber der tree is dann viel simpler und hat an besseren accuracy_score ??? 5/5 whys\n",
    "    # attributes.remove(best_attribute)\n",
    "    \n",
    "    # Build subtree for each attribute value\n",
    "    attribute_values = df[best_attribute].unique()\n",
    "    for att_value in attribute_values:\n",
    "        att_df = df.loc[df[best_attribute] == att_value]\n",
    "        \n",
    "        # If the new data frame is empty,\n",
    "        # the subtree is simply the most common value of the target attribute in the data frame\n",
    "        if len(att_df) == 0:\n",
    "            subtree = df[target_attribute].mode()[0]\n",
    "        else:\n",
    "            # Recursively call the function again with the new data frame and updated attributes\n",
    "            subtree = id3(att_df, attributes)\n",
    "        \n",
    "        # Now add the subtree to the above created tree\n",
    "        tree[best_attribute][att_value] = subtree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "# Prediction based on a given query {\"attribute_name_1\": \"attribute_value\", ...}, tree and default value\n",
    "# https://www.python-course.eu/Decision_Trees.php\n",
    "def predict(query, tree, default):\n",
    "    # Iterate through every attribute name of the query\n",
    "    for key in list(query.keys()):\n",
    "        # If the attribute name is in the list of values of the root node\n",
    "        if key in list(tree.keys()):\n",
    "            # Check if key and value from the query exist on the tree, return the default if not\n",
    "            try:\n",
    "                tree[key][query[key]] \n",
    "            except KeyError:\n",
    "                return default\n",
    "            \n",
    "            result = tree[key][query[key]]\n",
    "            \n",
    "            # if the result is another tree (subtree) we call the function again (recursive)\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query, result, default)\n",
    "            \n",
    "            else:\n",
    "                return result\n",
    "            \n",
    "       \n",
    "    \n",
    "breast_features = ['class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']\n",
    "breast_train = pd.read_csv('breast-cancer_train.data', names=breast_features)\n",
    "breast_test = pd.read_csv('breast-cancer_test.data', names=breast_features)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Calculate the tree as a dictionary with the ID3 algorithm by providing the data frame and features without 'class'\n",
    "breast_train_tree = id3(breast_train, breast_features[1:])\n",
    "\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# The most common value of the target attribute in the data frame\n",
    "breast_test_default = breast_test['class'].mode()[0]\n",
    "\n",
    "# Create a dictionary from the breast test data without the 'class' column\n",
    "breast_test_dict = breast_test.iloc[:, 1:].to_dict(orient = \"records\")\n",
    "\n",
    "y_true = breast_test['class'].tolist()\n",
    "y_pred = []\n",
    "\n",
    "for breast_query in breast_test_dict:\n",
    "    y_pred.append(predict(breast_query, breast_train_tree, breast_test_default))\n",
    "\n",
    "print('Accuracy = %.2f%%' % (accuracy_score(y_true, y_pred)*100))\n",
    "print('Time = %.2fs' % (end - start))\n",
    "#pprint.pprint(breast_train_tree)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%code\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Aufgabe 03.02"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Aufgabe 03.03\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}