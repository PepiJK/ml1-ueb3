{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uebung 03\n",
    "\n",
    "Gruppe: Josef Koch, Thomas Wally\n",
    "\n",
    "## Anmerkungen\n",
    "\n",
    "Für diese Übung ist es zugelassen, dass Sie in __Gruppen von zwei Personen__ arbeiten.\n",
    "Sie dürfen auch alleine arbeiten wenn Sie das bevorzugen.\n",
    "Beide Gruppenmitglieder müssen die Kreuzerlliste ausfüllen und die Ausarbeitung hochladen.\n",
    "Auch müssen die Namen beider Gruppenmitglieder in der Ausarbeitung vermerkt werden.\n",
    "\n",
    "Geben Sie bitte Quellen an wenn Sie Code aus dem Internet Kopieren.\n",
    "Laden Sie Ihre Lösung nur in privaten Repos auf Platformen wie GitHub, GitLab, usw. hoch.\n",
    "\n",
    "## Decision Tree und Random Forest\n",
    "\n",
    "In dieser Uebung sollen Sie einen Decision Tree mit Hilfe des ID3-Algorithmus selbst implementieren.\n",
    "Die Performance ihrer Implementierung sollen Sie mit der Implementierung einer Bibliothek vergleichen.\n",
    "Im Anschluss sollen Sie die Performance des Decision Trees mit der des Random Forest vergleichen.\n",
    "Zur Klassifizierung verwenden wir dieses Mal den [Breast Cancer Datensatz](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer).\n",
    "Die unten angegebenen Files `breast-cancer_train.data` und `breast-cancer_test.data` finden Sie im Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 03.01 (15 Punkte)\n",
    "\n",
    "Implementieren Sie selbststaendig einen Decision Tree.\n",
    "Verwenden Sie dazu den ID3-Algorithmus.\n",
    "\n",
    "- [x] Daten einlesen\n",
    "- [x] Decision Tree implementieren\n",
    "- [x] Decision Tree erstellen (trainieren mit `breast-cancer_train.data`)\n",
    "- [x] Accuracy mit `breast-cancer_test.data` berechnen\n",
    "- [x] Laufzeit des Training des Decision Tree angeben\n",
    "\n",
    "Sie muessen kein Pruning fuer den Baum implementieren.\n",
    "Sie muessen fuer diese Aufgabe keine _k_-Fold-Cross-Validation machen.\n",
    "\n",
    "Implementierungshinweise:\n",
    "* Fuer die Berechnung des Information Gain duerfen Sie Funktionen aus der `math` oder `numpy` Bibliotek verwenden.\n",
    "* Sie duerfen __pandas__(`pandas`) verwenden um die Daten einzulesen und zu speichern.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer die Berechnung der Accuracy verwenden.\n",
    "* Sie duerfen fuer den Decision Tree __keine__ Machine Learning Bibliothek verwenden (ausgenommen der oben angefuehrten)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aufgabe 03.02 (5 Punkte)\n",
    "\n",
    "Verwenden Sie eine fertige Implementierung des Decision Trees von __scikit-learn__(`sklearn`).\n",
    "\n",
    "- [x] Daten einlesen\n",
    "- [x] Decision Tree mit __scikit-learn__(`sklearn`) erstellen (trainieren mit `breast-cancer_train.data`)\n",
    "- [x] Accuracy mit `breast-cancer_test.data` berechnen\n",
    "- [x] Verlgeichen Sie die Accuracy Ihrer Implementierung der des Decision Tree den Sie mit __scikit-learn__ erstellt haben. Versuchen Sie den Unterschied in der Accuracy zu erklaeren.\n",
    "\n",
    "Implementierungshinweise:\n",
    "* Sie duerfen __pandas__(`pandas`) verwenden um die Daten einzulesen und zu speichern.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer die Berechnung der Accuracy verwenden.\n",
    "* Sie sollen __scikit-learn__(`sklearn`) fuer den Decision Tree verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aufgabe 03.03 (5 Bonuspunkte)\n",
    "\n",
    "Verwenden Sie eine fertige Implementierung des Random Forest von __scikit-learn__(`sklearn`).\n",
    "\n",
    "- [x] Daten einlesen\n",
    "- [x] Random Forest mit __scikit-learn__(`sklearn`) erstellen. Tunen Sie die Hyperparameter mit _k_-Fold-Cross-Validation mit `breast-cancer_train.data`.\n",
    "- [x] Accuracy mit `breast-cancer_test.data` berechnen\n",
    "- [x] Verlgeichen Sie die Accuracy des Decision Tree den Sie mit erstellt haben __scikit-learn__ mit der des Random Forest den Sie mit __scikit-learn__ erstellt haben. Versuchen Sie den Unterschied in der Accuracy zu erklaeren.\n",
    "\n",
    "Implementierungshinweise:\n",
    "* Sie duerfen __pandas__(`pandas`) verwenden um die Daten einzulesen und zu speichern.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer die Berechnung der Accuracy verwenden.\n",
    "* Sie duerfen __scikit-learn__(`sklearn`) fuer _k_-Fold-Cross-Validation verwenden.\n",
    "* Sie sollen __scikit-learn__(`sklearn`) fuer den Random Forest verwenden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anmerkungen\n",
    "\n",
    "Vergessen Sie nicht ihre Abgabe entsprechend zu dokumentieren.\n",
    "\n",
    "Laden Sie die von Ihnen bearbeitete `.ipynb` Datei sowie ein dazugehoeriges `requirements.txt` File in einer `.zip` Datei im Moodle Kurs hoch.\n",
    "Achten Sie darauf, dass keine Umlaute im Namen ihrer abgegebenen Files enthalten sind."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "#### Data Features\n",
    "1. Class: no-recurrence-events, recurrence-events\n",
    "2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "3. menopause: lt40, ge40, premeno.\n",
    "4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
    "5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
    "6. node-caps: yes, no.\n",
    "7. deg-malig: 1, 2, 3.\n",
    "8. breast: left, right.\n",
    "9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
    "10. irradiat: yes, no.\n",
    "\n",
    "### Aufgabe 03.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy = 66.23%\n",
      "Time = 6.02s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "import timeit\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the entropy of a given data frame column\n",
    "# Lecture03.pdf, page 34\n",
    "def calc_entropy(col):\n",
    "    values = col.unique()\n",
    "    ent = 0\n",
    "    for value in values:\n",
    "        # Amount of a specific value / total amount of values\n",
    "        pi = col.value_counts()[value] / len(col)\n",
    "        ent += - pi * math.log2(pi)\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Calculate the information gain of a given data frame and attribute\n",
    "# Lecture03.pdf, page 36\n",
    "def calc_gain(df, attribute, target_attribute = \"class\"):\n",
    "    total_ent = calc_entropy(df[target_attribute])\n",
    "    attribute_values = df[attribute].unique()\n",
    "    attribute_ent = 0\n",
    "    for value in attribute_values:\n",
    "        value_df = df.loc[df[attribute] == value]\n",
    "        entropy_df = calc_entropy(value_df[target_attribute])\n",
    "        value_ratio = len(value_df) / len(df)\n",
    "        attribute_ent += value_ratio * entropy_df\n",
    "    return total_ent - attribute_ent\n",
    "\n",
    "\n",
    "# Build the tree using the ID3 algorithm\n",
    "# https://en.wikipedia.org/wiki/ID3_algorithm\n",
    "def id3(df, attributes, target_attribute = \"class\"):\n",
    "    \n",
    "    # If there is only one value of the target attribute in the data frame, then return this value\n",
    "    if len(df[target_attribute].unique()) == 1:\n",
    "        return df[target_attribute].unique()[0]\n",
    "    \n",
    "    # If the attributes are empty, then return most common value of the target attribute in the data frame\n",
    "    if len(attributes) == 0:\n",
    "        return df[target_attribute].mode()[0]\n",
    "    \n",
    "    # Get the attribute with the highest information gain\n",
    "    gains = {}\n",
    "    for attribute in attributes:\n",
    "        gains[attribute] = calc_gain(df, attribute)\n",
    "    best_attribute = max(gains, key=gains.get)\n",
    "    \n",
    "    # Build the tree\n",
    "    tree = { best_attribute: {} }\n",
    "    \n",
    "    # Build subtree for each attribute value\n",
    "    attribute_values = df[best_attribute].unique()\n",
    "    for att_value in attribute_values:\n",
    "        att_df = df.loc[df[best_attribute] == att_value]\n",
    "        \n",
    "        # If the new data frame is empty,\n",
    "        # the subtree is simply the most common value of the target attribute in the data frame\n",
    "        if len(att_df) == 0:\n",
    "            subtree = df[target_attribute].mode()[0]\n",
    "        else:\n",
    "            # Remove the best attribute from the attributes array\n",
    "            attributes = [i for i in attributes if i != best_attribute]  \n",
    "    \n",
    "            # Recursively call the function again with the new data frame and updated attributes\n",
    "            subtree = id3(att_df, attributes)\n",
    "        \n",
    "        # Now add the subtree to the above created tree\n",
    "        tree[best_attribute][att_value] = subtree\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "# Prediction based on a given query {\"attribute_name_1\": \"attribute_value\", ...}, tree and default value\n",
    "# Inspiration: https://www.python-course.eu/Decision_Trees.php\n",
    "def predict(query, tree, default):\n",
    "    # Iterate through every attribute name of the query\n",
    "    for key in list(query.keys()):\n",
    "        # If the attribute name is in the list of values of the root node\n",
    "        if key in list(tree.keys()):\n",
    "            # Check if key and value from the query exist on the tree, return the default if not\n",
    "            try:\n",
    "                tree[key][query[key]] \n",
    "            except KeyError:\n",
    "                return default\n",
    "            \n",
    "            result = tree[key][query[key]]\n",
    "            \n",
    "            # if the result is another dictionary (tree) we call the function again (recursive)\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query, result, default)\n",
    "            \n",
    "            # if not we return the result (no-recurrence-events or recurrence-events)\n",
    "            else:\n",
    "                return result\n",
    "            \n",
    "       \n",
    "    \n",
    "breast_features = [\"class\", \"age\", \"menopause\", \"tumor-size\", \"inv-nodes\", \"node-caps\", \"deg-malig\", \"breast\", \"breast-quad\", \"irradiat\"]\n",
    "breast_train = pd.read_csv(\"breast-cancer_train.data\", names=breast_features)\n",
    "breast_test = pd.read_csv(\"breast-cancer_test.data\", names=breast_features)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Calculate the tree as a dictionary with the ID3 algorithm by providing the data frame and features without \"class\"\n",
    "breast_train_tree = id3(breast_train, breast_features[1:])\n",
    "\n",
    "end = timeit.default_timer()\n",
    "\n",
    "# The most common value of the target attribute in the data frame\n",
    "breast_test_default = breast_test[\"class\"].mode()[0]\n",
    "\n",
    "# Create a dictionary from the breast test data without the \"class\" column\n",
    "breast_test_dict = breast_test.iloc[:, 1:].to_dict(orient = \"records\")\n",
    "\n",
    "y_true = breast_test[\"class\"].tolist()\n",
    "y_pred = []\n",
    "\n",
    "for breast_query in breast_test_dict:\n",
    "    y_pred.append(predict(breast_query, breast_train_tree, breast_test_default))\n",
    "\n",
    "print(\"Accuracy = %.2f%%\" % (accuracy_score(y_true, y_pred)*100))\n",
    "print(\"Time = %.2fs\" % (end - start))\n",
    "#pprint.pprint(breast_train_tree)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%code\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Aufgabe 03.02\n",
    "\n",
    "Die Accuracy des sklearn Descion Trees ist deshalb schlechter weil hier zahlreiche optimierungen eingesetzt werden\n",
    "die zwar eine sehr kurze Laufzeit mit sich bringen, allerdings zur Folge haben das die Accuracy etwas darunter leidet.\n",
    "Auch sind hier die standard Werte des Decision Tree Classifier benutzt worden die natürlich auch nicht optimal sind."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy = 63.64%\n",
      "Time = 0.02s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import timeit\n",
    "\n",
    "\n",
    "\n",
    "# Encode the dataf rame values which are strings to integers to use them in the DecisionTreeClassifier from sklearn\n",
    "# Inspiration: http://chrisstrelioff.ws/sandbox/2015/06/08/decision_trees_in_python_with_scikit_learn_and_pandas.html\n",
    "def encode_dataframes(df_train, df_test, features):\n",
    "    feature_vals = {\n",
    "        \"class\": [\"no-recurrence-events\", \"recurrence-events\"], \n",
    "        \"age\": [\"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-99\"],\n",
    "        \"menopause\": [\"lt40\", \"ge40\", \"premeno\"],\n",
    "        \"tumor-size\": [\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\"],\n",
    "        \"inv-nodes\": [\"0-2\", \"3-5\", \"6-8\", \"9-11\", \"12-14\", \"15-17\", \"18-20\", \"21-23\", \"24-26\", \"27-29\", \"30-32\", \"33-35\", \"36-39\"],\n",
    "        \"node-caps\": [\"yes\", \"no\"],\n",
    "        \"deg-malig\": [1, 2, 3],\n",
    "        \"breast\": [\"left\", \"right\"],\n",
    "        \"breast-quad\": [\"left_up\", \"left_low\", \"right_up\", \"right_low\", \"central\"],\n",
    "        \"irradiat\": [\"yes\", \"no\"]\n",
    "    }\n",
    "    \n",
    "    df_train_mod = df_train.copy()\n",
    "    df_test_mod = df_test.copy()\n",
    "    \n",
    "    for feature in features:\n",
    "        # create a dictionary to map the string values (name) to their corresponding index in the array \n",
    "        map_to_int = {name: n for n, name in enumerate(feature_vals[feature])}\n",
    "        # replace the values of the train and test data frame with the map dictionary accordingly\n",
    "        df_train_mod[feature] = df_train_mod[feature].replace(map_to_int)\n",
    "        df_test_mod[feature] = df_test_mod[feature].replace(map_to_int)  \n",
    "        \n",
    "    return df_train_mod, df_test_mod\n",
    "\n",
    "\n",
    "\n",
    "breast_features = [\"class\", \"age\", \"menopause\", \"tumor-size\", \"inv-nodes\", \"node-caps\", \"deg-malig\", \"breast\", \"breast-quad\", \"irradiat\"]\n",
    "breast_train = pd.read_csv(\"breast-cancer_train.data\", names=breast_features)\n",
    "breast_test = pd.read_csv(\"breast-cancer_test.data\", names=breast_features)\n",
    "\n",
    "encoded_breast_train, encoded_breast_test = encode_dataframes(breast_train, breast_test, breast_features)\n",
    "\n",
    "y_train = encoded_breast_train[\"class\"]\n",
    "y_test = encoded_breast_test[\"class\"]\n",
    "X_train = encoded_breast_train[breast_features[1:]]\n",
    "X_test = encoded_breast_test[breast_features[1:]]\n",
    "\n",
    "start = timeit.default_timer()\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "end = timeit.default_timer()\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = %.2f%%\" % (accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Time = %.2fs\" % (end - start))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% code\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Aufgabe 03.03\n",
    "\n",
    "Natürlich hat der Random Forest mit hyper parameter tuning die beste Accuracy aber auch weil ein Random Forest\n",
    "sehr gute Ergebnisse mit einer kleinen Anzahl von Samples liefert und sich deshalb recht gut für unseren \n",
    "Trainingsdatensatz eignet. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy = 71.43%\n",
      "Time = 2.10s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "\n",
    "\n",
    "# Same as in Aufgabe 03.02\n",
    "def encode_dataframes(df_train, df_test, features):\n",
    "    feature_vals = {\n",
    "        \"class\": [\"no-recurrence-events\", \"recurrence-events\"], \n",
    "        \"age\": [\"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-99\"],\n",
    "        \"menopause\": [\"lt40\", \"ge40\", \"premeno\"],\n",
    "        \"tumor-size\": [\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\"],\n",
    "        \"inv-nodes\": [\"0-2\", \"3-5\", \"6-8\", \"9-11\", \"12-14\", \"15-17\", \"18-20\", \"21-23\", \"24-26\", \"27-29\", \"30-32\", \"33-35\", \"36-39\"],\n",
    "        \"node-caps\": [\"yes\", \"no\"],\n",
    "        \"deg-malig\": [1, 2, 3],\n",
    "        \"breast\": [\"left\", \"right\"],\n",
    "        \"breast-quad\": [\"left_up\", \"left_low\", \"right_up\", \"right_low\", \"central\"],\n",
    "        \"irradiat\": [\"yes\", \"no\"]\n",
    "    }\n",
    "    \n",
    "    df_train_mod = df_train.copy()\n",
    "    df_test_mod = df_test.copy()\n",
    "    \n",
    "    for feature in features:\n",
    "        # Create a dictionary to map the string values (name) to their corresponding index in the array \n",
    "        map_to_int = {name: n for n, name in enumerate(feature_vals[feature])}\n",
    "        # Replace the values of the train and test data frame with the map dictionary accordingly\n",
    "        df_train_mod[feature] = df_train_mod[feature].replace(map_to_int)\n",
    "        df_test_mod[feature] = df_test_mod[feature].replace(map_to_int)  \n",
    "        \n",
    "    return df_train_mod, df_test_mod\n",
    "\n",
    "\n",
    "# Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "def random_tuning(X_train, y_train):\n",
    "    rf = RandomForestRegressor(random_state = 42)\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = [\"auto\", \"sqrt\"]\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]# Create the random grid\n",
    "    random_grid = {\"n_estimators\": n_estimators,\n",
    "                   \"max_features\": max_features,\n",
    "                   \"max_depth\": max_depth,\n",
    "                   \"min_samples_split\": min_samples_split,\n",
    "                   \"min_samples_leaf\": min_samples_leaf,\n",
    "                   \"bootstrap\": bootstrap}\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    rf_random.best_params_\n",
    "    \n",
    "    # Results\n",
    "    \"\"\"\n",
    "    {\"n_estimators\": 400,\n",
    "     \"min_samples_split\": 2,\n",
    "     \"min_samples_leaf\": 4,\n",
    "     \"max_features\": \"sqrt\",\n",
    "     \"max_depth\": 10,\n",
    "     \"bootstrap\": True}\n",
    "    \"\"\"\n",
    "\n",
    "# Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 \n",
    "def grid_search_tuning(X_train, y_train):\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    # Based on the results of the random tuning\n",
    "    param_grid = {\n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [3, 5, 10, 20],\n",
    "        'max_features': [2, 5, 10],\n",
    "        'min_samples_leaf': [2, 4, 7],\n",
    "        'min_samples_split': [2, 4],\n",
    "        'n_estimators': [200, 400, 500, 700]\n",
    "    }# Create a based model\n",
    "    rf = RandomForestRegressor()# Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    # Results\n",
    "    \"\"\"\n",
    "    {'bootstrap': True, \n",
    "    'max_depth': 5, \n",
    "    'max_features': 2, \n",
    "    'min_samples_leaf': 4, \n",
    "    'min_samples_split': 4, \n",
    "    'n_estimators': 500}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "breast_features = [\"class\", \"age\", \"menopause\", \"tumor-size\", \"inv-nodes\", \"node-caps\", \"deg-malig\", \"breast\", \"breast-quad\", \"irradiat\"]\n",
    "breast_train = pd.read_csv(\"breast-cancer_train.data\", names=breast_features)\n",
    "breast_test = pd.read_csv(\"breast-cancer_test.data\", names=breast_features)\n",
    "\n",
    "encoded_breast_train, encoded_breast_test = encode_dataframes(breast_train, breast_test, breast_features)\n",
    "\n",
    "y_train = encoded_breast_train[\"class\"]\n",
    "y_test = encoded_breast_test[\"class\"]\n",
    "X_train = encoded_breast_train[breast_features[1:]]\n",
    "X_test = encoded_breast_test[breast_features[1:]]\n",
    "\n",
    "# Random hyper parameter tuning with 3 fold cross validation\n",
    "# takes about 2 minutes on my laptop (i5, 4 threads)\n",
    "#random_tuning(X_train, y_train)\n",
    "\n",
    "# Grid hyper parameter tuning with 3 fold cross validation\n",
    "# also takes about 2 minutes on my laptop (i5, 4 threads)\n",
    "#grid_search_tuning(X_train, y_train)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# Use the Results from the grid search tuning\n",
    "rf = RandomForestClassifier(n_estimators=500, min_samples_split=4, min_samples_leaf=4, max_features=2, max_depth=5, bootstrap=True)\n",
    "rf.fit(X_train, y_train)\n",
    "end = timeit.default_timer()\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = %.2f%%\" % (accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Time = %.2fs\" % (end - start))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}